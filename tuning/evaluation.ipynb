{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99215c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f635462",
   "metadata": {},
   "source": [
    "# Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee086223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "IMAGE_INFO = dict[str, str | int]\n",
    "IMAGE_DETECTIONS = dict[str, Any]\n",
    "BBOX = list[int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34d9f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data'\n",
    "\n",
    "TRAIN_IMAGES_PATH = f'{DATA_PATH}/train/images'\n",
    "TEST_IMAGES_PATH = f'{DATA_PATH}/test/images'\n",
    "\n",
    "TRAIN_INFO_PATH = f'{DATA_PATH}/metadata/metadata/iwildcam2022_train_annotations.json'\n",
    "TEST_INFO_PATH = f'{DATA_PATH}/metadata/metadata/iwildcam2022_test_information.json'\n",
    "\n",
    "DETECTIONS_INFO_PATH = f'{DATA_PATH}/metadata/metadata/iwildcam2022_mdv4_detections.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be7c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_INFO_PATH) as file:\n",
    "    train_info = json.load(file)\n",
    "\n",
    "train_images_info = train_info['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_INFO_PATH) as file:\n",
    "    test_info = json.load(file)\n",
    "\n",
    "test_images_info = test_info['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DETECTIONS_INFO_PATH) as file:\n",
    "    detections_info = json.load(file)\n",
    "\n",
    "image_detections_info = detections_info['images']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00255e8d",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017137c2",
   "metadata": {},
   "source": [
    "## Intersection over union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8018a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(y_true: list[BBOX], y_pred: list[BBOX]):\n",
    "    iou_res = 0\n",
    "    y_true = torch.tensor(y_true)\n",
    "    for y_p in y_pred:\n",
    "        y_p = torch.tensor(y_p).reshape(1,-1)\n",
    "        iou_res += float(metrics.bbox_iou(y_p,y_true,xywh=True).min())\n",
    "    return iou_res / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb2aa2",
   "metadata": {},
   "source": [
    "## Custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true: list[BBOX], y_pred: list[BBOX]):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    a = np.concatenate([y_pred,np.arange(len(y_pred)).reshape(-1,1)],axis=1)\n",
    "    b = np.concatenate([y_true,np.arange(len(y_true)).reshape(-1,1)],axis=1)\n",
    "\n",
    "    c = np.repeat(a,len(b),axis=0)\n",
    "    d = np.tile(b,(len(a),1))\n",
    "\n",
    "    res = np.concatenate([c,d],axis=1)\n",
    "    diff = ((res[:,[0,1]] - res[:,[5,6]])**2).sum(axis=1)**0.5\n",
    "\n",
    "    selected = [[],[]]\n",
    "    res_mse = 0\n",
    "    for i in np.argsort(diff):\n",
    "        if (res[i][4] not in selected[0]) & (res[i][9]  not in selected[1]):\n",
    "            selected[0].append(res[i][4])\n",
    "            selected[1].append(res[i][9])\n",
    "\n",
    "            point = ((res[i][[0,1]]-res[i][[5,6]])**2).sum()\n",
    "            box = ((res[i][[2,3]]-res[i][[7,8]])**2).sum()\n",
    "\n",
    "            res_mse += (point + box)**0.5\n",
    "        res_mse /= len(selected)\n",
    "    return res_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf9b75",
   "metadata": {},
   "source": [
    "## Absolute value of difference between number of ground-truth objects and number of predicted objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0541388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_found(y_true: list[BBOX], y_pred: list[BBOX]) -> int:\n",
    "    return np.abs(len(y_true) - len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79e7fa",
   "metadata": {},
   "source": [
    "## Metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_list, pred_list):\n",
    "    acc_res = {\"iou\":[],\"mse_box\":[],\"acc_obj\":[]}\n",
    "    \n",
    "    for y_true, y_pred in zip(true_list, pred_list):\n",
    "        \n",
    "        # one of the values is empty\n",
    "        if (len(y_true) != 0 and len(y_pred) == 0) or (len(y_true) == 0 and len(y_pred) != 0):\n",
    "            acc_res[\"iou\"].append(0)\n",
    "            acc_res[\"mse_box\"].append(1)\n",
    "            acc_res[\"acc_obj\"].append(num_found_obj(y_true, y_pred))\n",
    "\n",
    "        elif len(y_true) == 0 and len(y_pred) == 0:\n",
    "            acc_res[\"iou\"].append(1)\n",
    "            acc_res[\"mse_box\"].append(0)\n",
    "            acc_res[\"acc_obj\"].append(0)\n",
    "\n",
    "        else:\n",
    "            acc_res[\"iou\"].append(iou_metric(y_true, y_pred))\n",
    "            acc_res[\"mse_box\"].append(mse_box(y_true, y_pred))\n",
    "            acc_res[\"acc_obj\"].append(num_found_obj(y_true, y_pred))\n",
    "    \n",
    "    return acc_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf5243",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4316cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pred_labels(pred_dir: str, image_id: str) -> list[BBOX]:\n",
    "    bboxes = []\n",
    "    \n",
    "    file_path = f'{pred_dir}/{image_id}.txt'\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lns = file.readlines()\n",
    "            for l in lns:\n",
    "                pred = list(map(float, l.split(' ')))\n",
    "                bboxes.append(pred[1:])\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "        \n",
    "def get_pred_labels(pred_directory: str, images_info: list[IMAGE_INFO]) -> list[list[BBOX]]:\n",
    "    labels = []\n",
    "\n",
    "    for image_info in images_info:\n",
    "        image_id = image_info['id']\n",
    "        bbox = load_pred_labels(pred_directory, image_id)\n",
    "        labels.append(bbox)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47be1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_labels(images_info: list[IMAGE_INFO], detections: dict[str, IMAGE_DETECTIONS]) -> list[list[BBOX]]:\n",
    "    labels = []\n",
    "    for image_info in images_info:\n",
    "        image_id = image_info['id']\n",
    "        bboxes = get_bboxes(image_id, detections)\n",
    "        labels.append(bboxes)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PATH = 'yolov5/runs/detect/exp/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e31d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_pred_labels(LABELS_PATH, test_images_info)\n",
    "y_test = get_true_labels(test_images_info, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(evaluate(y_test, y_pred))\n",
    "evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
