{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "# !pip install -qr ./yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov5 import train, val, detect\n",
    "from yolov5.utils import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data consists of the following\n",
    "\n",
    "- train - directory with train images\n",
    "- test - directory with test images\n",
    "- iwildcam2022_train_annotations.json - file with train images metadata\n",
    "- iwildcam2022_test_information.json - file with test images metadata\n",
    "- iwildcam2022_mdv4_detections.json - file with detections data\n",
    "\n",
    "\n",
    "- processed_train.csv - file with train images metadata and labels (will be generated on load section)\n",
    "- processed_test.csv - file with test images metadata and labels (will be generated on load section)\n",
    "\n",
    "On data folder it is expected\n",
    "- data.yaml -  which store location of images path\n",
    "\n",
    "pretrained weights in format .pt\n",
    "- weights.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA = '../../data'\n",
    "TRAIN_IMAGES_LOCATION = DATA + '/train/images'\n",
    "TEST_IMAGES_LOCATION = DATA + '/test/images'\n",
    "\n",
    "DEST_DATA_LOCATION = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_label(filename: str, labels: list[dict]):\n",
    "    row = next((x for x in labels if filename == x['file']), [None])\n",
    "    if row is None:\n",
    "        return row\n",
    "    detections = row['detections']\n",
    "    filtered_detections = list(filter(lambda x: x['category'] == '1' and x['conf'] > 0.5, detections))\n",
    "    bboxes = list(map(lambda x: x['bbox'], filtered_detections))\n",
    "    return json.dumps(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a pytorch dataset we extract filenames from json file. Extracting label was a harder, because we need to extract one json and for every record we need to take filename and match it with record's filename from another json(basically it is similar to SQL `JOIN ON`), taking needed fields, filter by 'category' and confidence, and converting it to json string, for easy reading and saving. We save a progress in csv file, so we can always read it, instead of executing everything again. Dataset consist of taking image paths from our csv, reading it, resizing it, converting to rgb, and converting it to tensors. Right now dataset consist of images(that's why we use results from a pretrained model, as they are written for images, instead of `train_sequence_counts`), later it may change to sequences of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_data(images_json_path: str, labels_json_path: str, train: bool = True):\n",
    "    d = None\n",
    "    with open(labels_json_path) as f:\n",
    "        d = json.load(f)\n",
    "    labels = d['images']\n",
    "\n",
    "    with open(images_json_path) as f:\n",
    "        d = json.load(f)\n",
    "    images = json_normalize(data=d['images'],\n",
    "                   meta=['seq_num_frames', 'location', 'datetime', 'id', 'seq_id', 'width', 'height', 'file_name', 'sub_location', 'seq_frame_num'])\n",
    "    get_path = lambda x: get_label(filename=(('train/' if train else 'test/') + x['file_name']), labels=labels)\n",
    "    images['box'] = images.apply(get_path, axis=1)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = get_data(\n",
    "    DATA + '/metadata/metadata/iwildcam2022_train_annotations.json',\n",
    "    DATA + '/metadata/metadata/iwildcam2022_mdv4_detections.json',\n",
    "    train=True\n",
    ")\n",
    "df.to_csv('processed_train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = get_data(\n",
    "    DATA + '/metadata/metadata/iwildcam2022_test_information.json',\n",
    "    DATA + '/metadata/metadata/iwildcam2022_mdv4_detections.json',\n",
    "    train=False\n",
    ")\n",
    "df.to_csv('processed_test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_num_frames</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>id</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>...</th>\n",
       "      <th>height</th>\n",
       "      <th>file_name</th>\n",
       "      <th>sub_location</th>\n",
       "      <th>seq_frame_num</th>\n",
       "      <th>box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-06-05 05:44:19.000</td>\n",
       "      <td>8b02698a-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>30048d32-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>...</td>\n",
       "      <td>1080</td>\n",
       "      <td>8b02698a-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 0.091, 0.983, 0.876]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-06-05 05:44:20.000</td>\n",
       "      <td>8e5b81de-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>30048d32-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>...</td>\n",
       "      <td>1080</td>\n",
       "      <td>8e5b81de-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.193, 0.261, 0.803, 0.714]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_num_frames  location                 datetime  \\\n",
       "0               6         3  2013-06-05 05:44:19.000   \n",
       "1               6         3  2013-06-05 05:44:20.000   \n",
       "\n",
       "                                     id                                seq_id  \\\n",
       "0  8b02698a-21bc-11ea-a13a-137349068a90  30048d32-7d42-11eb-8fb5-0242ac1c0002   \n",
       "1  8e5b81de-21bc-11ea-a13a-137349068a90  30048d32-7d42-11eb-8fb5-0242ac1c0002   \n",
       "\n",
       "   ...  height                                 file_name sub_location  \\\n",
       "0  ...    1080  8b02698a-21bc-11ea-a13a-137349068a90.jpg          0.0   \n",
       "1  ...    1080  8e5b81de-21bc-11ea-a13a-137349068a90.jpg          0.0   \n",
       "\n",
       "   seq_frame_num                             box  \n",
       "0              0      [[0, 0.091, 0.983, 0.876]]  \n",
       "1              1  [[0.193, 0.261, 0.803, 0.714]]  \n",
       "\n",
       "[2 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed_train.csv', sep='\\t', index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>id</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>location</th>\n",
       "      <th>width</th>\n",
       "      <th>...</th>\n",
       "      <th>file_name</th>\n",
       "      <th>seq_frame_num</th>\n",
       "      <th>seq_num_frames</th>\n",
       "      <th>sub_location</th>\n",
       "      <th>box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>8b31d3be-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>a91ebc18-0cd3-11eb-bed1-0242ac1c0002</td>\n",
       "      <td>20</td>\n",
       "      <td>1280</td>\n",
       "      <td>...</td>\n",
       "      <td>8b31d3be-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[0, 0.55, 0.558, 0.438]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>8cf202be-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>a91ebc18-0cd3-11eb-bed1-0242ac1c0002</td>\n",
       "      <td>20</td>\n",
       "      <td>1280</td>\n",
       "      <td>...</td>\n",
       "      <td>8cf202be-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[0, 0.557, 0.684, 0.431]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   height                                    id  \\\n",
       "0    1024  8b31d3be-21bc-11ea-a13a-137349068a90   \n",
       "1    1024  8cf202be-21bc-11ea-a13a-137349068a90   \n",
       "\n",
       "                                 seq_id  location  width  ...  \\\n",
       "0  a91ebc18-0cd3-11eb-bed1-0242ac1c0002        20   1280  ...   \n",
       "1  a91ebc18-0cd3-11eb-bed1-0242ac1c0002        20   1280  ...   \n",
       "\n",
       "                                  file_name seq_frame_num  seq_num_frames  \\\n",
       "0  8b31d3be-21bc-11ea-a13a-137349068a90.jpg             0              10   \n",
       "1  8cf202be-21bc-11ea-a13a-137349068a90.jpg             1              10   \n",
       "\n",
       "   sub_location                         box  \n",
       "0           NaN   [[0, 0.55, 0.558, 0.438]]  \n",
       "1           NaN  [[0, 0.557, 0.684, 0.431]]  \n",
       "\n",
       "[2 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed_test.csv', sep='\\t', index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.1, 0.1, 0.3, 0.4]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_str_to_list(str_list: str):\n",
    "    \"\"\"Converts string with labels to (taken from csv file) list format\"\"\"\n",
    "\n",
    "    res_list = []\n",
    "    i = 1\n",
    "    while i < len(str_list):\n",
    "        start = 0\n",
    "        # if '[' found then new labels started\n",
    "        if str_list[i]=='[':\n",
    "            start = i+1\n",
    "            # store all characters before ']'\n",
    "            while str_list[i]!=']':\n",
    "                i+=1\n",
    "            end = i\n",
    "            #  if string collected in while is not\n",
    "            # empty then convert string to array\n",
    "            if start!=end:\n",
    "                res_list.append(list( \n",
    "                                map(float,\n",
    "                                str_list[start:end].split(', ')) \n",
    "                            ))\n",
    "        i+=1\n",
    "    #if labels is not found store empty list\n",
    "    if len(res_list)==0:\n",
    "        return []\n",
    "    \n",
    "    return res_list\n",
    "\n",
    "#example\n",
    "convert_str_to_list(\"[[1, 0.1, 0.1, 0.3, 0.4]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(path, image_name, bbox_list):\n",
    "    \"\"\"Saves labels in 'image_name.txt\"\"\"\n",
    "    \n",
    "    # take image name\n",
    "    label_file_name = image_name.split('.')[0] + '.txt'\n",
    "    # create file\n",
    "    with open(path+label_file_name, 'w') as f:\n",
    "        #save in format: \"class x y w h\"\n",
    "        for l in bbox_list:\n",
    "            line = \"1\"\n",
    "\n",
    "            for i in l:\n",
    "                line = line+\" \"+ str(i)\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take labels from\n",
    "def process_label(box_string, file_name, dest_folder):\n",
    "    bbox = convert_str_to_list(box_string)\n",
    "    load_labels(dest_folder,file_name,bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(f_name, init_folder, dest_folder):\n",
    "    \"\"\"\n",
    "    Copies image with name f_name from init folder to dest_folder.\n",
    "    Returns true if image file is found and copied\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isfile(f'{init_folder}/{f_name}'):# and check_image(f'./{init_folder}/{f_name}'):\n",
    "        os.system(f'cp {init_folder}/{f_name} {dest_folder}/{f_name}')\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>85877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>68458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1280</td>\n",
       "      <td>1024</td>\n",
       "      <td>42987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>2882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     width  height  counts\n",
       "279   2048    1536   85877\n",
       "83    1920    1080   68458\n",
       "39    1280    1024   42987\n",
       "0     1280     720    2882"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statistic of common image sizes\n",
    "df = pd.read_csv('processed_train.csv', sep='\\t', index_col=0)\n",
    "\n",
    "stat_h_w = {\n",
    "    \"width\": [],\n",
    "    \"height\": [],\n",
    "    \"counts\": []\n",
    "}\n",
    "\n",
    "hw = df[['height','width']]\n",
    "for h in np.unique(df['height']):\n",
    "    for w in np.unique(df['width']):\n",
    "        stat_h_w[\"width\"].append(w)\n",
    "        stat_h_w[\"height\"].append(h)\n",
    "        stat_h_w[\"counts\"].append(((hw['height']==h) & (hw['width']==w)).sum())\n",
    "\n",
    "stat_h_w = pd.DataFrame(stat_h_w)    \n",
    "stat_h_w.sort_values(by=['counts'],ascending=False).head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DEST_DATA_LOCATION}/images/train\"):\n",
    "    os.makedirs(f\"{DEST_DATA_LOCATION}/images/train\")\n",
    "\n",
    "if not os.path.exists(f\"{DEST_DATA_LOCATION}/images/val\"):\n",
    "    os.makedirs(f\"{DEST_DATA_LOCATION}/images/val\")\n",
    "\n",
    "if not os.path.exists(f\"{DEST_DATA_LOCATION}/images/test\"):\n",
    "    os.makedirs(f\"{DEST_DATA_LOCATION}/images/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DEST_DATA_LOCATION}/labels/train\"):\n",
    "    os.makedirs(f\"{DEST_DATA_LOCATION}/labels/train\")\n",
    "\n",
    "if not os.path.exists(f\"{DEST_DATA_LOCATION}/labels/val\"):\n",
    "    os.makedirs(f\"{DEST_DATA_LOCATION}/labels/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset_Descriptor stores data about images:\n",
    "\n",
    "path to datasets:\n",
    "DATASET_LOCATION\n",
    "DATASET_INIT_LOCATION\n",
    "DATASET_DEST_LOCATION\n",
    "\n",
    "path to each image\n",
    "self.image_dir\n",
    "labels for each image\n",
    "self.labels\n",
    "number of samples\n",
    "self.sample_size\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class Dataset_Descriptor:\n",
    "    DATASET_LOCATION = \"dataset\"\n",
    "    LOAD_TYPES = [\"train\",\"val\",\"test\"]\n",
    "    DATASET_INIT_LOCATION = {\n",
    "        \"train\": TRAIN_IMAGES_LOCATION,\n",
    "        \"val\": TRAIN_IMAGES_LOCATION,\n",
    "        \"test\": TEST_IMAGES_LOCATION\n",
    "    }\n",
    "    DATASET_DEST_LOCATION = {\n",
    "        \"images\": {\n",
    "            \"train\": f\"{DEST_DATA_LOCATION}/images/train\",\n",
    "            \"val\": f\"{DEST_DATA_LOCATION}/images/val\",\n",
    "            \"test\": f\"{DEST_DATA_LOCATION}/images/test\"\n",
    "        },\n",
    "        \"labels\": {\n",
    "            \"train\": f\"{DEST_DATA_LOCATION}/labels/train/\",\n",
    "            \"val\": f\"{DEST_DATA_LOCATION}/labels/val/\",\n",
    "            \"test\": f\"{DEST_DATA_LOCATION}/labels/test/\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, l_type, init_pos, sample_size):\n",
    "        # dataset_type [train,test,val]\n",
    "        self.l_type = l_type\n",
    "        # path to all images\n",
    "        self.image_dir = []\n",
    "        self.labels = []\n",
    "        self.seq_id = []\n",
    "        self.image_name = []\n",
    "        \n",
    "        self.pos = init_pos\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        # select images with given size\n",
    "        # load images to working directory and store files locations\n",
    "        if self.l_type in [\"train\",\"val\"]:\n",
    "            df = pd.read_csv('processed_train.csv', sep='\\t', index_col=0)\n",
    "            df = df[(df[\"height\"]==1080) & (df[\"width\"]==1920)]\n",
    "            df = df.reset_index(drop=True)\n",
    "        else:\n",
    "            df = pd.read_csv('processed_test.csv', sep='\\t', index_col=0)\n",
    "            df = df[(df[\"height\"]==1080) & (df[\"width\"]==1920)]\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "        self.pos = self.search_images(df=df, l_type=self.l_type, init_pos=self.pos, set_size=self.sample_size)\n",
    "    \n",
    "    def load_from_working_dir(self):\n",
    "        images_dir = [Dataset_Descriptor.DATASET_DEST_LOCATION['images']['train'],\n",
    "                      Dataset_Descriptor.DATASET_DEST_LOCATION['images']['test'],\n",
    "                      Dataset_Descriptor.DATASET_DEST_LOCATION['images']['val']]\n",
    "        labels_dir = [Dataset_Descriptor.DATASET_DEST_LOCATION['labels']['train'],\n",
    "                      Dataset_Descriptor.DATASET_DEST_LOCATION['labels']['val']]\n",
    "        for i in range(3):\n",
    "            for img in os.listdir(images_dir[i]):\n",
    "                self.image_dir.append(img)\n",
    "            labels_array = convert_str_to_list(df['box'][init_pos])\n",
    "            df['id']==img\n",
    "            labels_array = np.array(labels_array) if len(labels_array)>0 else np.array([])\n",
    "            self.labels.append(labels_array )\n",
    "            self.seq_id.append(df['seq_id'][init_pos])\n",
    "            self.image_name.append(df['id'][init_pos])\n",
    "    \n",
    "    def search_images(self, df, l_type, init_pos, set_size):\n",
    "        #get path to init dir\n",
    "        #get path to worling dir: images,labels\n",
    "        init_path = Dataset_Descriptor.DATASET_INIT_LOCATION[l_type]\n",
    "        dest_path_i = Dataset_Descriptor.DATASET_DEST_LOCATION[\"images\"][l_type]\n",
    "        dest_path_l = Dataset_Descriptor.DATASET_DEST_LOCATION[\"labels\"][l_type]\n",
    "        \n",
    "        #get first set_size images\n",
    "        while set_size > 0:\n",
    "            #load images\n",
    "            res = load_image(f_name = df['file_name'][init_pos],\n",
    "                       init_folder = init_path,\n",
    "                       dest_folder = dest_path_i)\n",
    "            if res:\n",
    "                self.image_dir.append(f\"{dest_path_i}/{df['file_name'][init_pos]}\")\n",
    "                labels_array = convert_str_to_list(df['box'][init_pos])\n",
    "                labels_array = np.array(labels_array) if len(labels_array)>0 else np.array([])\n",
    "                self.labels.append(labels_array )\n",
    "                self.seq_id.append(df['seq_id'][init_pos])\n",
    "                self.image_name.append(df['id'][init_pos])\n",
    "                \n",
    "                if l_type in [\"train\",\"val\"]:\n",
    "                    process_label(box_string = df['box'][init_pos],\n",
    "                                  file_name = df['file_name'][init_pos],\n",
    "                                  dest_folder = dest_path_l)\n",
    "                set_size-=1\n",
    "            init_pos += 1\n",
    "        return init_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_work_dir():\n",
    "    os.system(f\"rm -r {Dataset_Descriptor.DATASET_DEST_LOCATION['images']['train']}/*\")\n",
    "    os.system(f\"rm -r {Dataset_Descriptor.DATASET_DEST_LOCATION['images']['test']}/*\")\n",
    "    os.system(f\"rm -r {Dataset_Descriptor.DATASET_DEST_LOCATION['images']['val']}/*\")\n",
    "    os.system(f\"rm -r {Dataset_Descriptor.DATASET_DEST_LOCATION['labels']['train']}/*\")\n",
    "    os.system(f\"rm -r {Dataset_Descriptor.DATASET_DEST_LOCATION['labels']['val']}/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_descriptor_train = Dataset_Descriptor(l_type=\"train\", init_pos=0, sample_size=7000)\n",
    "data_descriptor_val = Dataset_Descriptor(l_type=\"val\", init_pos=data_descriptor_train.pos, sample_size=1000)\n",
    "data_descriptor_test = Dataset_Descriptor(l_type=\"test\", init_pos=0, sample_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5/yolov5s.pt, cfg=, data=data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=15, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=cuda:0, multi_scale=False, single_cls=True, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, tensorboard=True\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "YOLOv5 ðŸš€ v7.0-8-g350e8eb Python-3.10.6 torch-1.13.0+cu117 CUDA:0 (NVIDIA GeForce GTX 1050, 4040MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5/yolov5s.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00046875), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/maxim/Desktop/2022-2023/icv/computer-vision-project/final/\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/maxim/Desktop/2022-2023/icv/computer-vision-project/final/da\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.32 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to yolov5/runs/train/exp3/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolov5/runs/train/exp3\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/0       3.1G    0.08491     0.0312          0         34        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1000       1749      0.498      0.434      0.413      0.138\n",
      "\n",
      "1 epochs completed in 0.159 hours.\n",
      "Optimizer stripped from yolov5/runs/train/exp3/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from yolov5/runs/train/exp3/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating yolov5/runs/train/exp3/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       1000       1749      0.498      0.433      0.413      0.138\n",
      "Results saved to \u001b[1myolov5/runs/train/exp3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(weights='yolov5/yolov5s.pt', cfg='', data='data.yaml', hyp={'lr0': 0.01, 'lrf': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0, 'copy_paste': 0.0}, epochs=1, batch_size=15, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket='', cache=None, image_weights=False, device='cuda:0', multi_scale=False, single_cls=True, optimizer='SGD', sync_bn=False, workers=8, project='yolov5/runs/train', name='exp', exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias='latest', tensorboard=True, save_dir='yolov5/runs/train/exp3')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data.yaml'\n",
    "\n",
    "train.run(\n",
    "    data=data_path,\n",
    "    epochs=1,\n",
    "    batch_size=15,\n",
    "    single_cls=True,\n",
    "    optimizer=\"SGD\",\n",
    "    noplots=False,\n",
    "    tensorboard=True,\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that model is tuned with script with lr, optimizer, momentum tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
